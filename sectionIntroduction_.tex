\section{Introduction}
Auto-scaling is the process of dynamically allocating and deallocating resources for a particular application deployed in the cloud. This is of vital importance for clients to optimize their resource utilization. The goal of a cloud auto-scaling mechanism (i.e., auto-scaler) is to achieve higher Quality of Service (QoS) levels while minimizing the associated cost.

Cloud auto-scaling systems have to address two major challenges. First, an auto-scaler needs to be aware of the workload that the system has to deal with. Second, auto-scaler should allocate the right amount of resources to the system in a cost-effective manner while maintaining its QoS. Auto-scalers use two main approaches to address the first challenge. Depending on the selected approach, the auto-scaler would gain workload awareness in a proactive or reactive manner. In the reactive approach, the auto-scaling decision would be triggered by a predefined set of events. In contrast, proactive auto-scaling mechanisms forecast the workload ahead such that the auto-scaler can make decisions based on the anticipated workload instead of waiting for a trigger. A major challenge in cloud workload prediction is to come up with a solution that would perform well against drastically different workload patterns. For example, a model that typically performs well on workloads with seasonal trends does not perform well with frequently fluctuating loads.The reactive approach is commonly used in PaaS auto-scalers \cite{website:openshift} \cite{website:bluemix}. It is relatively simple to implement, but greatly reduces the solution space available when it comes to addressing the second problem, i.e.,resource allocation. Whereas  an accurate understanding about the future workload enables better resource management such as proactive spawning of new a virtual machine (VM) and not killing a VM until end of the billing cycle. Hence, we have opted for proactive auto-scaling over reactive auto-scaling in our solution, as proactive mechanisms supported by accurate prediction mechanism enable auto-scalers to make more detailed decisions.

Much research work has been conducted (as discussed in Section II) in relation to resource allocation problem as well \cite{Roy_2011} \cite{Iqbal_2011}. However, almost all the available PaaS solutions are built on rule-based scaling. In the rule-based approach, for example, spin up decisions will be taken when the average memory consumption of the cluster of VMs is over 75\%. Such mechanisms mostly rely on user defined threshold parameters for configuring policies or rules to govern the scaling decisions \cite{modeldriven}. One of the major drawbacks with this type of rule based, threshold-driven auto-scaling is that the user is expected to be a domain expert, capable of setting up appropriate threshold values for memory usage and CPU utilization for a given application. Also, mapping application metrics such as response time and throughput to system-level metrics such as CPU utilization and I/O Operation per Second (IOPS) is nontrivial.

In this paper, we propose a proactive and cost-aware auto-scaling solution to address these issues by combining a predictive model, cost model, and a smart killing feature. We utilize a workload prediction mechanism based on time series forecasting and machine learning techniques. Experimental results show that the proposed ensemble method outperforms individual techniques, as well as some of the popularly used ensemble models, when it comes to accuracy. Moreover, we propose a greedy heuristic-scaling algorithm to address the resource allocation problem considering both the QoS and cost factors. In the algorithm, we introduce the idea of \textit{penalty factors} for quantifying and incorporating performance degradations to the scaling model, an idea inspired by penalties introduced in Service Level Agreements (SLAs) of popular PaaS platforms such as Google App Engine. The scaling algorithm evaluates all possibilities and selects the optimum resource configuration considering the lease cost and penalty due to performance degradation. This scaling mechanism mitigates the problem of having to incorporate threshold values as in rule-based scaling.

In addition to introducing a scaling algorithm to calculate the required resources, we take a novel perspective in addressing the auto-scaling problem by injecting pricing model awareness to our solution. In our opinion, ignorance of the pricing model in the scaling decision is a major drawback in current PaaS auto-scalers. For example, consider a typical application deployed on Amazon Web Services (AWS). On a sudden fluctuation in the workload, a typical auto-scaler would scale-out by spawning a new VM instance, and when the workload is back to normal, the auto-scaler would scale-in by killing one of the VM instances which has already been paid for an hour. Thus, the customer has effectively lost ~50 minutes of utilization of the instance, though it has been paid. The \textit{smart killing} feature, which we adapted to our solution from \cite{Bunch_2012} would apprehend the utilization of each VM instance, and scale-in instances only when their lease periods are about to expire. It could result in extra saving by mitigating the requirement to spin up another instance on a sudden fluctuation of the workload within the paid hour.

In an attempt to implement our solution and demonstrate the utility of the proposed PaaS auto-scaler, we target Apache Stratos PaaS (however, the techniques detailed here are extensible to other PaaS systems as well). Apache Stratos is an open source PaaS framework that encapsulates IaaS-level details to the level of reduced granularity and complexity of a PaaS, while offering multi-tenancy and multi-cloud deployment capabilities. Stratos supports multiple IaaS providers, including AWS, OpenStack, GCE (Google Compute Engine), and Microsoft Azure\cite{website:stratos}. Stratos auto-scaler is based on policy-driven decisions, which performs workload prediction for a small time window (usually a few minutes) but does not utilize resource optimization approaches explicitly, thereby incurring unnecessary costs to the customer, such as over provisioning of resources and naive scale-down decisions. Apart from the typical characteristics of a PaaS auto-scaler, Stratos offers other features such as a modular architecture allowing easy modifications, and the availability of a mock IaaS as a component for testing and evaluation, which would greatly assist any research in terms of monetary and implementation cost.

We evaluated the proposed solution, namely inteliScaler, by deploying Apache Stratos on AWS Elastic Compute Cloud (EC2). We deployed the three-tier bidding benchmark RUBiS as the user application and experimented with several workload traces. Our results demonstrate that inteliScaler successfully scales the application in response to fluctuating workloads, without user intervention and without off-line profiling. More importantly, we compare our solution with existing rule-based triggers and show that inteliScaler is superior to such approaches.

Rest of the paper is organized as follows. Section II presents the related work. High-level architecture of inteliScaler is presented in Section III. Prediction model is presented in Section IV while cost-aware resource allocation is presented in Section V. Section VI and VII present simulation and experimental results, respectively. Concluding remarks are presented in Section VIII.