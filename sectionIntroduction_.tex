\section{Introduction}
Autoscaling is the process of dynamically allocating and deallocating resources for particular application deployed in the cloud. This is of vital importance for clients to optimize their resource utilization.  The goal of a cloud autoscaling mechanism (autoscaler) is to achieve higher Quality of Service (QoS) levels while minimizing the associated cost. \\

We have identified two major challenges faced by cloud autoscaling systems. Firstly, an autoscaler needs to be aware of the workload that the system has to deal with. Secondly, autoscaler should  allocate the right amount of resources to the system in a cost-effective manner while preserving its QoS.\\

There are two main approaches in cloud computing domain for addressing the first challenge. Depending on the selected approach, the auto scaler would gain workload awareness in a proactive or reactive manner. In the reactive approach, the autoscaling decision would be triggered by a predefined set of events. In contrast, proactive autoscaling mechanisms forecast the workload ahead so the autoscaler can make decisions based on the anticipated workload instead of waiting for a trigger. A major challenge in cloud workload prediction is to come up with a solution that would perform well against different workload patterns. For example, a model that performs well on workloads with seasonal trends will not perform well with frequently fluctuating loads.\\

We have opted for proactive autoscaling over reactive autoscaling in our solution, because proactive mechanisms supported by accurate prediction mechanism enable autoscalers to make more detailed decisions. The reactive approach used in most PaaS autoscalers is much simple, but greatly reduces the solution space available when addressing the second (resource allocation) problem. In our case, we utilize a workload prediction mechanism based on time series forecasting and machine learning techniques. Results have shown that the ensemble method used by us outperforms individual techniques as well as some of the popularly used ensemble models in terms of accuracy.\\

Much research work has been conducted (as discussed in Section2) in relation to resource allocation problem as well. However, almost all the available PaaS solutions are built on rule based scaling. In the rule-based approach, for example, spin up decisions will be taken when the average memory consumption of the cluster of virtual machines is over 75\%. Such mechanisms mostly rely on user defined threshold parameters. One of the major drawbacks in this type of rule-based threshold-driven autoscaling is that the user is expected to be a domain expert, capable of setting up threshold values such as memory usage and CPU consumption for his application, in order for configuring policies or rules to govern the scaling decisions \cite{modeldriven}. On the other hand, mapping application metrics such as response time and transactions per unit time to system level metrics such as CPU usage and disk I/O rates is a research level problem, which an average user cannot comprehend.\\

In this paper we propose a greedy heuristic scaling algorithm considering both QoS as well as cost factors. In the algorithm we introduce the idea of \textit{penalty factors} for quantifying and incorporating performance degradation to the scaling model, an idea inspired by penalties introduced in Service Level Agreements (SLAs) of popular PaaS platforms such as Google App Engine. The scaling algorithm evaluates all possibilities and selects the optimum resource configuration considering the lease cost and penalty due to performance degradation. This scaling mechanism mitigates the problem of having to incorporate threshold values as in rule based scaling.\\

In addition to introducing a scaling algorithm to calculate required resources, we take a novel perspective in addressing the autoscaling problem by injecting pricing model awareness to our solution. In our opinion, ignorance of the pricing model in scaling decision making is major drawback in the currently available PaaS autoscalers. For a motivation example, let us consider a typical application deployed on Amazon Web Service (AWS). On a sudden fluctuation in the workload, a typical auto scaler would scale-out to spin up a new VM instance, and when the workload is back to normal, the auto-scaler would scale-in by blindly killing one of the VM instances which has already been paid for, for an hour. Thus the customer has effectively lost 50 minutes of utilization of the instance in terms of payment. The smart Killing feature which we have adapted to our solution from [Appscale] would apprehend the utilization of each VM instance and scale-in instances only when their lease periods are about to expire. It would also save an extra cost by mitigating the requirement to spin up another instance on a sudden fluctuation of the workload within the paid hour. \\

In an attempt to implement our solution and demonstrate the effects of a PaaS autoscaler, we target Apache Stratos PaaS; however, the techniques detailed here are extensible to other PaaS systems as well. Apache Stratos is an open source PaaS framework that encapsulates IaaS-level details to the level of reduced granularity and complexity of a PaaS, while offering multi-tenancy and multi-cloud deployment capabilities. Stratos supports multiple IaaS providers, including AWS, OpenStack, GCE (Google Compute Engine) and Microsoft Azure\cite{website:stratos}. Stratos autoscaler is based on policy-driven decisions, which performs workload prediction for a small time window (usually a few minutes) but does not utilize resource optimization approaches explicitly, thereby incurring unnecessary costs to the customer, such as overprovisioning of resources and naive scale down decisions. Apart from the typical characteristics of a PaaS auto scaler, Stratos offers other features such as a modular architecture allowing easy modifications, and the availability of a mock IaaS as a component for testing and evaluation, which would greatly assist any research in terms of monetary and implementation cost.\\

We evaluate our solution inteliScaler by deploying  Apache Stratos on AWS Elastic Compute Cloud (EC2). We deploy the three-tier bidding benchmark RUBiS as the user application and experiment with various workload traces. Our results demonstrate that inteliScaler successfully scales the application in response to fluctuating workloads, without user intervention and without offline profiling. More importantly, we compare our solution with existing rule based triggers and show that inteliScaler is superior to such approaches.\\
