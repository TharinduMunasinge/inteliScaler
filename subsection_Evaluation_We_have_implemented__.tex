\subsection{Evaluation}

We have implemented the proposed prediction algorithm in R and tested it against some publicly available datasets as well as several real-world cloud workloads \cite{AutoscaleAnalyser} collected from server applications. Our solution was compared with each of the constituent models in the ensemble solution, two other popular ensemble techniques (mean and median ensemble), the existing prediction technique in Apache Stratos, and the prediction model described in \cite{Roy_2011}.

Table \ref{prediction_cloud} includes a comparison of prediction approaches against CPU and memory usage traces obtained from a dedicated standalone server from a private cloud, and a summarized portion of the Google cluster dataset \cite{GoogleClusterData} which includes a series of task execution details against their starting and ending times.

\begin{table}[]
\label{prediction_cloud}
\centering
\caption{Comparison of performance on cloud datasets}
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Model}} & \multicolumn{2}{c|}{Google Cluster}                   & \multicolumn{2}{c|}{Memory}                              & \multicolumn{2}{c|}{CPU}                              \\ \cline{2-7} 
\multicolumn{1}{|c|}{}                       & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAPE} & \multicolumn{1}{c|}{RMSE} & \multicolumn{1}{c|}{MAPE} \\ \hline
ARIMA                                        & 12.963                    & 0.051                     & 7.238                     & 0.136                     & 2.976                     & 0.036                     \\ \hline
Exponential                                  & 12.886                    & 0.041                     & 7.005                     & 0.160                     & 3.150                     & {\ul 0.048}               \\ \hline
Neural net.                                  & 12.530                    & 0.036                     & 8.169                     & 0.135                     & \textbf{2.792}            & 0.031                     \\ \hline
Stratos                                      & {\ul 19.757}              & {\ul 0.116}               & {\ul 9.928}               & 0.172               & {\ul 5.692}               & 0.024            \\ \hline

ARMA-based model                                      & 12.549              & 0.069               & 7.185               & {\ul 0.180}               & 3.477               & \textbf{0.023}            \\ \hline

Mean Ensemble                                      & 12.099              & 0.051               & 7.036               & 0.130               & 2.900               & 0.029            \\ \hline
Median Ensemble                                      &  12.059              & 0.055               &  7.010               & 0.141               & 2.944               & 0.028            \\ \hline

Proposed Ensemble                                     & \textbf{11.934}           & \textbf{0.027}            & \textbf{6.972}            & \textbf{0.129}            & 2.873                     & 0.027                     \\ \hline
\end{tabular}
\end{table}

Boldfaced and underlined values in Table I denote instances of the best and worst performances observed for each dataset across all methods, respectively. From the results it is clear that our proposed technique provides the best prediction results for several datasets while never performing worse than the individual prediction methods.